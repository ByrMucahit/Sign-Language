{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import The necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils, vgg16\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from shutil import copyfile\n",
    "import math\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Mucahit BAYAR\\\\Document\\\\Deep Learning\\\\Transfer_Learning\\\\Sign Language'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have Raw data Firstly, We dont'need to resize because of All images have same size but then Dividing into train, test, validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"Dataset/Dataset\"\n",
    "os.listdir(base_dir)\n",
    "\n",
    "for file in os.listdir(base_dir):\n",
    "    data_img = base_dir+\"/\"+file\n",
    "    #img = Image.open(data_img)\n",
    "    #print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---> 205\n",
      "1 ---> 206\n",
      "2 ---> 206\n",
      "3 ---> 206\n",
      "4 ---> 207\n",
      "5 ---> 207\n",
      "6 ---> 207\n",
      "7 ---> 206\n",
      "8 ---> 208\n",
      "9 ---> 204\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"{} ---> {}\".format(i,len(os.listdir(base_dir+'/'+str(i)+'/')))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(base_dir)) # Title Of DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(base_dir) # Passing Titles To Variables Named Classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/0', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/1', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/2', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/3', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/4', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/5', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/6', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/7', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/8', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/9']\n"
     ]
    }
   ],
   "source": [
    "source_path = [f'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/{a}' for a in classes] # Assigning List That Folder OF within Dataset to Array . \n",
    "print(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_dir',\n",
       " '1_dir',\n",
       " '2_dir',\n",
       " '3_dir',\n",
       " '4_dir',\n",
       " '5_dir',\n",
       " '6_dir',\n",
       " '7_dir',\n",
       " '8_dir',\n",
       " '9_dir']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_dir=[f'{a}_dir' for a in classes] # We got two folders such as Dog and Cat, In this snippet we have maden name of file.\n",
    "classes_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_dir :  205\n",
      "1_dir :  206\n",
      "2_dir :  206\n",
      "3_dir :  206\n",
      "4_dir :  207\n",
      "5_dir :  207\n",
      "6_dir :  207\n",
      "7_dir :  206\n",
      "8_dir :  208\n",
      "9_dir :  204\n"
     ]
    }
   ],
   "source": [
    "for cl_dir, cl_path in zip(classes_dir, source_path): # Viewing length of files within DataSet\n",
    "    print(cl_dir,': ',len(os.listdir(cl_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating-Folder\n",
    "try:\n",
    "    os.mkdir('C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training')\n",
    "    os.mkdir('C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation')\n",
    "    os.mkdir('C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/testing')\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/0', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/1', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/2', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/3', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/4', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/5', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/6', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/7', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/8', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/9']\n",
      "\n",
      " ['C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/0', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/1', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/2', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/3', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/4', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/5', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/6', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/7', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/8', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/9']\n",
      "\n",
      " ['C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/0', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/1', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/2', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/3', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/4', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/5', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/6', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/7', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/8', 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/9']\n"
     ]
    }
   ],
   "source": [
    "TRAINING_PATH= 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training' # Path Of Training folder\n",
    "VALIDATION_PATH= 'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation' # Path Of Validating Folder\n",
    "training_dir_path=[f'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/{a}' for a in classes] # Opened Folders up with in training folder such as Dog And Cat.\n",
    "print('\\n',training_dir_path)\n",
    "validation_dir_path=[f'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/{a}' for a in classes] # Opened Folders up with in validating folder such as Dog And Cat.\n",
    "print('\\n',validation_dir_path)\n",
    "testing_dir_path=[f'C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet//testing/{a}' for a in classes] # Opened Folders up with in testing folder such as Dog And Cat.\n",
    "print('\\n',testing_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Folder\n",
    "for train_dir_path in training_dir_path:\n",
    "    try:\n",
    "        os.mkdir(train_dir_path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Folder\n",
    "for val_dir_path in validation_dir_path:\n",
    "    try:\n",
    "        os.mkdir(val_dir_path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Folder\n",
    "for test_dir_path in testing_dir_path:\n",
    "    try:\n",
    "        os.mkdir(test_dir_path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, VALIDATION, TESTING, SPLIT_SIZE): # We got four variables to any processing.\n",
    "    files = []\n",
    "    print('Split Data')\n",
    "    for filename in os.listdir(SOURCE):# traveling within Source folder.\n",
    "        file = SOURCE + '/'+ filename # Adding any  folder that what that has Current name.\n",
    "        if os.path.getsize(file) > 0: # If File has size is greater than zero\n",
    "            files.append(filename) # adding folder to  within the another folder\n",
    "        else: # Size equal zero\n",
    "            print(filename + \"is zero length, so ignoring.\")\n",
    "            \n",
    "    training_length = int( len(files)* SPLIT_SIZE) # Adjusting length Of training dataset\n",
    "    validation_length = int(len(files) * 0.10) # Adjusting length Of validation dataset\n",
    "    testing_length = int(len(files) - training_length - validation_length) #Adjusting length Of training dataset\n",
    "    \n",
    "    # Viewing\n",
    "    print('SOURCE: ', SOURCE, '\\n TRAINING', TRAINING, '\\n VALIDATION',VALIDATION, '\\n ',len(files))\n",
    "    print('training_lenght:', training_length)\n",
    "    print('validation_length:',validation_length)\n",
    "    print('testing_length: ',testing_length)\n",
    "    \n",
    "    #Assignin Size Processing\n",
    "    shuffled_Set = random.sample(files, len(files))\n",
    "    training_set = shuffled_Set[0:training_length]\n",
    "    validation_set = shuffled_Set[training_length:(training_length+validation_length)]\n",
    "    testing_set = shuffled_Set[:testing_length]\n",
    "    \n",
    "    print(len(training_set))\n",
    "    print(len(validation_set))\n",
    "    print(len(testing_set))\n",
    "    \n",
    "    # Set Data to Relatively Folder\n",
    "    for filename in training_set:\n",
    "        this_file = SOURCE + '/'+ filename\n",
    "        destination = TRAINING +'/'+ filename\n",
    "        copyfile(this_file, destination)\n",
    "        \n",
    "    for filename in validation_set:\n",
    "        this_file= SOURCE +'/'+ filename\n",
    "        destination = VALIDATION+'/'+filename\n",
    "        copyfile(this_file, destination)\n",
    "    for filename in testing_set:\n",
    "        this_file = source +'/'+ filename\n",
    "        destination = TESTING + '/' + filename\n",
    "        copyfile(this_file, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/0 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/0 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/0 \n",
      "  205\n",
      "training_lenght: 174\n",
      "validation_length: 20\n",
      "testing_length:  11\n",
      "174\n",
      "20\n",
      "11\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/1 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/1 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/1 \n",
      "  206\n",
      "training_lenght: 175\n",
      "validation_length: 20\n",
      "testing_length:  11\n",
      "175\n",
      "20\n",
      "11\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/2 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/2 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/2 \n",
      "  206\n",
      "training_lenght: 175\n",
      "validation_length: 20\n",
      "testing_length:  11\n",
      "175\n",
      "20\n",
      "11\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/3 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/3 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/3 \n",
      "  206\n",
      "training_lenght: 175\n",
      "validation_length: 20\n",
      "testing_length:  11\n",
      "175\n",
      "20\n",
      "11\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/4 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/4 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/4 \n",
      "  207\n",
      "training_lenght: 175\n",
      "validation_length: 20\n",
      "testing_length:  12\n",
      "175\n",
      "20\n",
      "12\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/5 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/5 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/5 \n",
      "  207\n",
      "training_lenght: 175\n",
      "validation_length: 20\n",
      "testing_length:  12\n",
      "175\n",
      "20\n",
      "12\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/6 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/6 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/6 \n",
      "  207\n",
      "training_lenght: 175\n",
      "validation_length: 20\n",
      "testing_length:  12\n",
      "175\n",
      "20\n",
      "12\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/7 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/7 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/7 \n",
      "  206\n",
      "training_lenght: 175\n",
      "validation_length: 20\n",
      "testing_length:  11\n",
      "175\n",
      "20\n",
      "11\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/8 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/8 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/8 \n",
      "  208\n",
      "training_lenght: 176\n",
      "validation_length: 20\n",
      "testing_length:  12\n",
      "176\n",
      "20\n",
      "12\n",
      "Splitting \n",
      "\n",
      "Split Data\n",
      "SOURCE:  C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/Dataset/9 \n",
      " TRAINING C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/training/9 \n",
      " VALIDATION C:/Users/Mucahit BAYAR/Document/Deep Learning/Transfer_Learning/Sign Language/DataSet/validation/9 \n",
      "  204\n",
      "training_lenght: 173\n",
      "validation_length: 20\n",
      "testing_length:  11\n",
      "173\n",
      "20\n",
      "11\n",
      "Splitting \n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_size = .85 # Ratio Of Split Size\n",
    "for source, train_dir_path, val_dir_path, test_dir_path in zip(source_path,training_dir_path, validation_dir_path, testing_dir_path): # Rooting within folder\n",
    "    split_data(source, train_dir_path,val_dir_path, test_dir_path, split_size) # Starting function by Assigning Variables\n",
    "    print('Splitting \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datas Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"Dataset/training\"\n",
    "test_path = \"Dataset/testing\"\n",
    "validation_path = \"Dataset/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1748 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator().flow_from_directory(train_path,\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_batches = ImageDataGenerator().flow_from_directory(validation_path,\n",
    "                                                        target_size=(224,224),\n",
    "                                                        batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = ImageDataGenerator().flow_from_directory(test_path,\n",
    "                                                       target_size=(224,224),\n",
    "                                                       shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top = False, input_shape = (224, 224, 3), pooling = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 14,719,818\n",
      "Trainable params: 7,084,554\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "last_output = base_model.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(Adam(lr=0.0001), loss= 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 29s 2s/step - loss: 3.9373 - accuracy: 0.1833 - val_loss: 2.0954 - val_accuracy: 0.2667\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 1.8190 - accuracy: 0.3389 - val_loss: 1.7065 - val_accuracy: 0.3667\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 1.4823 - accuracy: 0.4722 - val_loss: 1.4753 - val_accuracy: 0.5333\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 29s 2s/step - loss: 1.0834 - accuracy: 0.6444 - val_loss: 0.8985 - val_accuracy: 0.7444\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 30s 2s/step - loss: 0.8038 - accuracy: 0.7111 - val_loss: 0.8698 - val_accuracy: 0.6667\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 29s 2s/step - loss: 0.6561 - accuracy: 0.7667 - val_loss: 0.4732 - val_accuracy: 0.8667\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.4110 - accuracy: 0.8722 - val_loss: 0.3868 - val_accuracy: 0.8333\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.3442 - accuracy: 0.9222 - val_loss: 0.3786 - val_accuracy: 0.8778\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.1570 - accuracy: 0.9556 - val_loss: 0.2821 - val_accuracy: 0.8889\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.1466 - accuracy: 0.9722 - val_loss: 0.2045 - val_accuracy: 0.9111\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 29s 2s/step - loss: 0.1704 - accuracy: 0.9556 - val_loss: 0.3454 - val_accuracy: 0.8889\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.2003 - accuracy: 0.9111 - val_loss: 0.2212 - val_accuracy: 0.9222\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 29s 2s/step - loss: 0.0988 - accuracy: 0.9778 - val_loss: 0.1518 - val_accuracy: 0.9444\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.1244 - accuracy: 0.9833 - val_loss: 0.0710 - val_accuracy: 0.9889\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 29s 2s/step - loss: 0.0658 - accuracy: 0.9889 - val_loss: 0.1962 - val_accuracy: 0.9333\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 29s 2s/step - loss: 0.0696 - accuracy: 0.9667 - val_loss: 0.1251 - val_accuracy: 0.9667\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.0270 - accuracy: 0.9944 - val_loss: 0.4022 - val_accuracy: 0.9111\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.1329 - accuracy: 0.9722 - val_loss: 0.2846 - val_accuracy: 0.9111\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.0952 - accuracy: 0.9667 - val_loss: 0.4474 - val_accuracy: 0.8778\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 28s 2s/step - loss: 0.0964 - accuracy: 0.9722 - val_loss: 0.1817 - val_accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "history = new_model.fit_generator(train_batches, \n",
    "                                  steps_per_epoch=18, \n",
    "                                  validation_data= valid_batches, \n",
    "                                  validation_steps=3, \n",
    "                                  epochs=20, \n",
    "                                  verbose=1, \n",
    "                                  callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "# This function's task is divide test data set into files and targets.\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets= np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "\n",
    "test_files, test_targets = load_dataset(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function's task is convert from image to array.\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0) # Expanding Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function's task is make list image as array.\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)] # tqdm is show loading that how is Processing forward as visual\n",
    "    return np.vstack(list_of_tensors) # concatenation Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 431.98it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tensors = preprocess_input(paths_to_tensor(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 2s/step - loss: 0.1365 - accuracy: 0.9737\n",
      "\n",
      "Testing loss: 0.1365\n",
      "Testing accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Model Performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "cm = confusion_matrix(np.argmax(test_targets, axis=1),\n",
    "                     np.argmax(new_model.predict(test_tensors), axis=1))\n",
    "\n",
    "plt.imshow(cm, cmap= plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "indexes = np.arange(len(cm_labels))\n",
    "\n",
    "for i in indexes:\n",
    "    for j in indexes:\n",
    "        plt.text(j, i, cm[j,i])\n",
    "plt.xticks(indexes, cm_labels, rotation= 90)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.yticks(indexes, cm_labels)\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
